{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617049b2-520c-4ab0-a337-ad12ee051462",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Vindhya Hegde\n",
    "\n",
    "### ST 590"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d53591-e41d-4d81-b9ba-c43aea3535df",
   "metadata": {},
   "source": [
    "# Purpose of the report\n",
    "\n",
    "This purpose of this report is to create a machine learning model using PySpark's MLlib module and use it to forecast incoming data streams. The dataset utilized in this study is from the UCI machine learning repository dataset, which also includes information on time of day, temperature, humidity, and electricity consumption data from various areas of Tetouan city. This project aims to demonstrate the use of PySpark's robust tools and libraries to create and deploy a machine learning model in a streaming context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14111e99-aaae-4a6e-8445-3fb75e150dd1",
   "metadata": {},
   "source": [
    "# Fitting Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a4394f-c9fc-4cee-9bbf-60575ffd1b81",
   "metadata": {},
   "source": [
    "The data (https://www4.stat.ncsu.edu/~online/datasets/power_ml_data.csv) is read into a standard pandas data frame using the pd.read_csv() function and converted this to a spark data frame. Imported neccessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f864c2a3-3763-4907-91a4-8c70c3281fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Read data into a pandas dataframe\n",
    "df_pandas = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/power_ml_data.csv\")\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PowerData\").getOrCreate()\n",
    "\n",
    "# Convert pandas dataframe to a Spark dataframe\n",
    "df_spark = spark.createDataFrame(df_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3ddb1-c761-4b4b-ae16-e0e09d639140",
   "metadata": {},
   "source": [
    "Found means, standard deviations, min, max, and median values for all the variables by using `.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a4f3a8-f122-4f1d-89e7-6de6f108b09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+---------------------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|       Temperature|          Humidity|        Wind_Speed|General_Diffuse_Flows|     Diffuse_Flows|      Power_Zone_1|      Power_Zone_2|      Power_Zone_3|            Month|              Hour|\n",
      "+-------+------------------+------------------+------------------+---------------------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|             47174|             47174|             47174|                47174|             47174|             47174|             47174|             47174|            47174|             47174|\n",
      "|   mean|18.813219803281438| 68.28839827023359|1.9616214016194993|   182.53118043837654| 74.98721140882624|32335.168690495833| 21027.20497598019|17831.197607816728|6.510599058803578|11.488383431551279|\n",
      "| stddev| 5.813341359553941|15.560330479134192|2.3493511404671956|   264.43185588803766|124.25614632084834| 7130.013305333635|5199.7871528123005|6622.5904698693585|3.437367004368705| 6.921920450693015|\n",
      "|    min|             3.247|             11.34|              0.05|                0.004|             0.011|        13895.6962|       8560.081466|        5935.17407|                1|                 0|\n",
      "|    25%|             14.42|             58.32|             0.078|                0.062|             0.122|       26287.59156|       16954.95868|       13120.32686|                4|                 5|\n",
      "|    50%|             18.78|             69.89|             0.086|                4.779|             4.246|       32259.82301|        20802.9106|       16405.28211|                7|                11|\n",
      "|    75%|             22.91|              81.5|             4.915|                318.7|             101.0|       37313.96061|       24694.93671|       21628.91566|                9|                17|\n",
      "|    max|             40.01|              94.8|             6.483|               1163.0|             936.0|       52146.85905|       37408.86076|       47598.32636|               12|                23|\n",
      "+-------+------------------+------------------+------------------+---------------------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997edd7-b0e5-43e3-b661-a0e72652b692",
   "metadata": {},
   "source": [
    "â€“ Found the correlations between all variables by using `.corr()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00837447-a5fa-470c-b44b-b924e01c6902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----------------------------+--------------------------+------------------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------+\n",
      "|corr(Temperature, Humidity)|corr(Temperature, Wind_Speed)|corr(Humidity, Wind_Speed)|corr(General_Diffuse_Flows, Diffuse_Flows)|corr(Power_Zone_1, Power_Zone_2)|corr(Power_Zone_1, Power_Zone_3)|corr(Power_Zone_2, Power_Zone_3)|   corr(Month, Hour)|\n",
      "+---------------------------+-----------------------------+--------------------------+------------------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------+\n",
      "|        -0.4601429412885557|           0.4764210955803555|      -0.13612147371170583|                        0.5645303710201623|              0.8346940579474785|              0.7506555678809356|              0.5723439832303366|-4.19363425066390...|\n",
      "+---------------------------+-----------------------------+--------------------------+------------------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "df_spark.select(corr(\"Temperature\", \"Humidity\"), corr(\"Temperature\", \"Wind_Speed\"), corr(\"Humidity\", \"Wind_Speed\"), \n",
    "                corr(\"General_Diffuse_Flows\", \"Diffuse_Flows\"),\n",
    "               corr(\"Power_Zone_1\", \"Power_Zone_2\"), corr(\"Power_Zone_1\", \"Power_Zone_3\"), corr(\"Power_Zone_2\", \"Power_Zone_3\"), \n",
    "                corr(\"Month\", \"Hour\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace55d1f-a0f3-4a4b-8702-8e77b542c412",
   "metadata": {},
   "source": [
    "Created a one-way contingency table of the Month variable by using `groupBy()` and `.agg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a7accf-5d85-4d94-9eb6-51040fb18092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Month|Count|\n",
      "+-----+-----+\n",
      "|    1| 4014|\n",
      "|    2| 3588|\n",
      "|    3| 4057|\n",
      "|    4| 3893|\n",
      "|    5| 3997|\n",
      "|    6| 3913|\n",
      "|    7| 4029|\n",
      "|    8| 3999|\n",
      "|    9| 3913|\n",
      "|   10| 4026|\n",
      "|   11| 3877|\n",
      "|   12| 3868|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "# Create a one-way contingency table of the Month variable\n",
    "month_oneway = df_spark.groupBy('Month').agg(count('*').alias('Count')).orderBy('Month')\n",
    "month_oneway.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f954f10-a902-4025-b6d1-1cc34ec868d9",
   "metadata": {},
   "source": [
    "Created a one-way contingency table of the Hour variable by using `groupBy()` and `.agg()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9adcc09a-30ca-481b-9757-9a3a53f46bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Hour|Count|\n",
      "+----+-----+\n",
      "| 0.0| 1950|\n",
      "| 1.0| 1973|\n",
      "| 2.0| 1973|\n",
      "| 3.0| 1966|\n",
      "| 4.0| 1986|\n",
      "| 5.0| 1968|\n",
      "| 6.0| 1992|\n",
      "| 7.0| 1964|\n",
      "| 8.0| 1957|\n",
      "| 9.0| 1976|\n",
      "|10.0| 1955|\n",
      "|11.0| 1972|\n",
      "|12.0| 1979|\n",
      "|13.0| 1956|\n",
      "|14.0| 1971|\n",
      "|15.0| 1947|\n",
      "|16.0| 1950|\n",
      "|17.0| 1979|\n",
      "|18.0| 1955|\n",
      "|19.0| 1950|\n",
      "|20.0| 1945|\n",
      "|21.0| 1976|\n",
      "|22.0| 1966|\n",
      "|23.0| 1968|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a one-way contingency table of the Hour variable\n",
    "hour_oneway = df_spark.groupBy('Hour').agg(count('*').alias('Count')).orderBy('Hour')\n",
    "hour_oneway.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621f5dd-f3d0-4589-ac70-b44aa4af6aa5",
   "metadata": {},
   "source": [
    "Created a two-way contingency table for the Month and Hour variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ae7129-d565-419d-8f31-cd42773e387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|Month|Hour|Count|\n",
      "+-----+----+-----+\n",
      "|    1|   0|  161|\n",
      "|    1|   1|  165|\n",
      "|    1|   2|  168|\n",
      "|    1|   3|  162|\n",
      "|    1|   4|  167|\n",
      "|    1|   5|  168|\n",
      "|    1|   6|  168|\n",
      "|    1|   7|  168|\n",
      "|    1|   8|  172|\n",
      "|    1|   9|  174|\n",
      "|    1|  10|  164|\n",
      "|    1|  11|  174|\n",
      "|    1|  12|  170|\n",
      "|    1|  13|  165|\n",
      "|    1|  14|  167|\n",
      "|    1|  15|  163|\n",
      "|    1|  16|  169|\n",
      "|    1|  17|  170|\n",
      "|    1|  18|  168|\n",
      "|    1|  19|  162|\n",
      "+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "# Create a two-way contingency table of the Month and Hour variables\n",
    "two_way = df_spark.groupBy('Month', 'Hour').agg(count('*').alias('Count')).orderBy('Month', 'Hour')\n",
    "two_way.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f6e7f-eae1-4bb0-b8c1-f30d614d6a55",
   "metadata": {},
   "source": [
    "Grouped the data by Month and found the means of the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6efbd181-103a-456b-9f3d-015cda465410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------+-------------------+--------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|Month|  Temperature_mean|    Humidity_mean|    Wind_Speed_mean|General_Diffuse_Flows_mean|Diffuse_Flows_mean| Power_Zone_1_mean| Power_Zone_2_mean| Power_Zone_3_mean|         Hour_mean|\n",
      "+-----+------------------+-----------------+-------------------+--------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|    1|12.734699053313424|68.25854758345791| 0.7022234678624802|        103.95965819631209| 69.79882635774754| 31052.98442787007|19407.916365649475| 17736.35168477324| 11.51270553064275|\n",
      "|    2|12.656535117056842|66.49092530657751| 1.1139765886287718|        125.47113545150515|  92.3306151059088|30973.863159715755|18774.586005972385|17309.707870419465|11.497491638795987|\n",
      "|    3|14.584054720236646|71.11588365787539| 1.0060172541286598|        181.40171949716375| 93.15590460931698|31162.869031165836|18459.612112786035|16945.462800258043|11.479171801824007|\n",
      "|    5|20.301401050788073|68.60932199149357|   2.30747285464099|        274.50002601951394|122.76557593194893| 32379.46046425318|19973.085386786082|17604.282564152116|11.466599949962472|\n",
      "|    4|16.414754687901336|75.40817621371694|  0.222989725147702|         157.7222427433853| 83.49453686103197|31143.206765884937|17600.306571434827|18574.918338348314| 11.48214744413049|\n",
      "|    7|27.200593199304986|57.59948374286429|  4.641781831719927|         294.1120372300817| 75.41053760238238| 35805.53043592462| 24130.02818170255|   28175.034099032| 11.49565649044428|\n",
      "|    6| 22.13270636340399|68.76125990288782| 1.5613462816253483|         277.4345330948132|  103.227789164323|34573.227025573666|20649.034589747287| 20416.13009093538|11.456682852031689|\n",
      "|    8|25.740415103775955|66.02262065516375|  4.533251062765713|        227.17863515878855| 67.10584746186578| 36436.26165143788|24657.024551832943|24684.368960552885|11.506626656664166|\n",
      "|    9|22.640564784053243|66.86830564784064|  2.947096089956558|         202.2016335292607| 49.07062202913375|33415.103455862554|20189.459836739075|14928.415530158953|  11.5323281369793|\n",
      "|   10|20.476249379036258| 71.5240163934426| 2.7842208147044194|         115.8145561351204|46.628719324391646| 32806.99279622949| 21457.89000130904| 13266.43733663137|11.486587183308496|\n",
      "|   11|16.819407531596593| 69.6383415011607|  1.258901728140313|        121.91511916430152| 62.87613257673453|28993.342195633202| 23229.11746956413| 12867.07310019397|11.492133092597369|\n",
      "|   12|13.283378490175783|69.23738624612214|0.25532445708376034|        100.02325051706269| 34.30538210961792| 28959.10286604968| 23618.56451605991|11017.120312390387|11.452430196483972|\n",
      "+-----+------------------+-----------------+-------------------+--------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Group the data by Month and find the means of the numeric variables\n",
    "df_month_means = df_spark.groupBy('Month').agg(mean('Temperature').alias('Temperature_mean'),\n",
    "                                               mean('Humidity').alias('Humidity_mean'),\n",
    "                                               mean('Wind_Speed').alias('Wind_Speed_mean'),\n",
    "                                               mean('General_Diffuse_Flows').alias('General_Diffuse_Flows_mean'),\n",
    "                                               mean('Diffuse_Flows').alias('Diffuse_Flows_mean'),\n",
    "                                               mean('Power_Zone_1').alias('Power_Zone_1_mean'),\n",
    "                                               mean('Power_Zone_2').alias('Power_Zone_2_mean'),\n",
    "                                               mean('Power_Zone_3').alias('Power_Zone_3_mean'),\n",
    "                                               mean('Hour').alias('Hour_mean'))\n",
    "\n",
    "df_month_means.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce5968-e1f6-455d-b2aa-77e2d654cd22",
   "metadata": {},
   "source": [
    "Grouped the data by Month and found the standard deviations of the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1938c3a6-c842-492a-8e60-002099127524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|Month|    Temperature_SD|       Humidity_SD|     Wind_Speed_SD|General_Diffuse_Flows_SD|  Diffuse_Flows_SD|   Power_Zone_1_SD|   Power_Zone_2_SD|   Power_Zone_3_SD|           Hour_SD|\n",
      "+-----+------------------+------------------+------------------+------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|    1|3.2406352202253177| 12.15616997395505|1.6117952014385526|      166.16470976772894|  131.459171585262| 7402.323410683091| 4515.295696366688| 4436.997404934671|6.8933864667013784|\n",
      "|    2| 2.619715133289512|12.411941927246787|1.9811568615798605|      206.73018017500877|169.15551710161583| 6874.584790780019| 4390.391100519847|4353.9759463094815| 6.900280948950495|\n",
      "|    3|3.7588517619095954|13.918146099336553|1.9009817904056068|      260.14888917166206|151.16792344997845| 6782.136538982632| 4185.117594550106|  4256.76554626955| 6.934685602294895|\n",
      "|    5|3.2999517830264136|16.436022749116013|2.4083281081182726|      331.99889739474145|171.58594340324657| 6809.332811001886|4182.5436720014095| 4353.394233957315|6.9047355594893345|\n",
      "|    4| 2.806220593833569|14.312654855624078|0.8203433573472342|      246.17350131285014|123.91235156483745| 6496.700166744174|3835.6293841116035| 4556.263192196545|  6.91394782479513|\n",
      "|    7|3.8567073301651567|18.849986112632102|1.1105390901773047|       331.7336077428813| 95.04451651686645| 6966.074191037386|  4968.51110111139|  6913.95836073139| 6.927292532034068|\n",
      "|    6|2.6897243987839956|14.972905335787273|2.2354105856986264|        328.277209820488| 143.4978962074959| 7317.808097492525| 4465.664309494708| 5596.702925763702| 6.933494099313091|\n",
      "|    8| 2.949887327569401|18.482550994912017|1.3008996273361944|       289.9061923113907| 90.66254000257548| 7054.722275396859| 5163.442764556622| 6520.955954895705| 6.950722974866449|\n",
      "|    9|2.8775846322752625| 15.99282606989947|2.2934124256572472|       270.1725641702132| 67.52353874901077|  6471.36690049191|4205.5042189416745|3425.8563203955623| 6.922189515744973|\n",
      "|   10| 2.987292863085969|13.980791119840983|2.3987026068167046|       185.0432984170808| 69.42130815062337|6479.1334820123275| 4612.671174818133|3087.8058176327845| 6.944603798274296|\n",
      "|   11|3.6832250625264473|12.832010541598438| 2.075376528780303|      184.69202456862132|123.14150471869303| 5918.582563224674| 5452.892157923399| 3509.671953716189| 6.933768382211351|\n",
      "|   12| 3.348618746384049|13.739463409846879|0.8937554493017353|      161.70244063920802| 56.39356877719106| 6176.465110237933| 5713.467068605237|2840.3432089859934| 6.910305828988747|\n",
      "+-----+------------------+------------------+------------------+------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "# Group the data by Month and find the standard deviations of the numeric variables\n",
    "month_SD = df_spark.groupBy('Month').agg(stddev('Temperature').alias('Temperature_SD'),\n",
    "                                                 stddev('Humidity').alias('Humidity_SD'),\n",
    "                                                 stddev('Wind_Speed').alias('Wind_Speed_SD'),\n",
    "                                                 stddev('General_Diffuse_Flows').alias('General_Diffuse_Flows_SD'),\n",
    "                                                 stddev('Diffuse_Flows').alias('Diffuse_Flows_SD'),\n",
    "                                                 stddev('Power_Zone_1').alias('Power_Zone_1_SD'),\n",
    "                                                 stddev('Power_Zone_2').alias('Power_Zone_2_SD'),\n",
    "                                                 stddev('Power_Zone_3').alias('Power_Zone_3_SD'),\n",
    "                                                stddev('Hour').alias('Hour_SD'))\n",
    "\n",
    "month_SD.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4d73e-4411-4f57-8399-8c1e7d03e76a",
   "metadata": {},
   "source": [
    "Used a spark SQL data frame method to cast the variable as a DoubleType using `.withColumn()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbdb8cee-84a7-4837-981f-6fe896dd75c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Temperature: double, Humidity: double, Wind_Speed: double, General_Diffuse_Flows: double, Diffuse_Flows: double, Power_Zone_1: double, Power_Zone_2: double, Power_Zone_3: double, Month: bigint, Hour: double]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df_spark = df_spark.withColumn(\"Hour\", df_spark[\"Hour\"].cast(DoubleType()))\n",
    "print(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f30ca-7cbf-4fa3-9a91-371f976ae241",
   "metadata": {},
   "source": [
    "In the next code, The sequence of the transform were as follows:\n",
    "\n",
    "+ `Binarizer()` - In this code, doing the transformation `Binarizer()` with inputCol = \"Hour\" and outputCol = \"day_or_night\" \n",
    "\n",
    "+ `OneHotEncoder()` - In this code, created `OneHotEncoder()` estimator to convert Month into vector using inputCol = \"Month\" and outputCol= \"MonthVec\" \n",
    "\n",
    "+ `SQLTransformer()` - In this code, doing the transformation `SQLTransformer()` and select the features from the dataset and get Power_Zone_3 as label\n",
    "\n",
    "+ `VectorAssembler()` - In this code, doing the transformation `VectorAssembler()` outputCol = \"features\" \n",
    "\n",
    "+ `PCA()` - In this code, doing the transformation `PCA()` with inputCol = \"features\" and outputCol = \"pca_features\" \n",
    "\n",
    "+ `VectorAssembler()` - Again used `VectorAssembler()` for selecting inputCol = \"pca_features\", \"day_or_night\", \"Power_Zone_1\", \"Power_Zone_2\", \"MonthVec\" and outputCol = \"features_new\" where it was selected as predictors\n",
    "\n",
    "This was done by first fitting the transformer using `.fit()` and then using `.transform()` to transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "019e14a9-091c-418d-9ce3-dda23710a9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Temperature: double, Humidity: double, Wind_Speed: double, General_Diffuse_Flows: double, Diffuse_Flows: double, Power_Zone_1: double, Power_Zone_2: double, day_or_night: double, MonthVec: vector, label: double, features: vector, pca_features: vector, features_new: vector]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer, SQLTransformer\n",
    "binarizer = Binarizer(threshold=6.5, inputCol=\"Hour\", outputCol=\"day_or_night\")\n",
    "binarizer.transform(df_spark)\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "# create OneHotEncoder estimator\n",
    "encoder = OneHotEncoder(inputCols=[\"Month\"], outputCols=[\"MonthVec\"])\n",
    "# fit and transform the data\n",
    "encoder.fit(binarizer.transform(df_spark)).transform(binarizer.transform(df_spark))\n",
    "\n",
    "#sqltransformer\n",
    "sqltrans = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                 SELECT Temperature, Humidity, Wind_Speed, General_Diffuse_Flows, Diffuse_Flows,Power_Zone_1,Power_Zone_2, day_or_night, MonthVec,\n",
    "                 Power_Zone_3 as label FROM __THIS__\n",
    "                 \"\"\"\n",
    ")\n",
    "sqltrans.transform(encoder.fit(binarizer.transform(df_spark)).transform(binarizer.transform(df_spark)))\n",
    "\n",
    "#vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"Temperature\", \"Humidity\", \"Wind_Speed\", \"General_Diffuse_Flows\", \"Diffuse_Flows\"], outputCol=\"features\")\n",
    "assembler.transform(sqltrans.transform(encoder.fit(binarizer.transform(df_spark)).transform(binarizer.transform(df_spark))))\n",
    "\n",
    "#pca\n",
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=5, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "assembled_data = assembler.transform(sqltrans.transform(encoder.fit(binarizer.transform(df_spark)).transform(binarizer.transform(df_spark))))\n",
    "pca_fit = pca.fit(assembled_data)\n",
    "pca_fit.transform(assembled_data)\n",
    "\n",
    "#vector assembler2\n",
    "assembler2 = VectorAssembler(inputCols=[\"pca_features\", \"day_or_night\", \"Power_Zone_1\", \n",
    "                                        \"Power_Zone_2\", \"MonthVec\"],\n",
    "                             outputCol=\"features_new\")\n",
    "assembler2.transform(pca_fit.transform(assembled_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed7cce-221f-4d1a-a920-d106992f23dd",
   "metadata": {},
   "source": [
    "Imported necessary libraries to fit an elastic net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06e16c4d-e609-4721-94b1-005f82f85279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7b055-0280-4f3e-9b6b-0f5df0f8612b",
   "metadata": {},
   "source": [
    "In this section elastic net model was fit. The steps involved:\n",
    "\n",
    "+ Defined the Linear Regression function by `LinearRegression()`\n",
    "+ Defined the pipeline and stages included transformations\n",
    "+ Defined the parameter grid for crossvaliadtion by using `ParamGridBuilder()` and `.addGrid()` to specify the tuning parameter values `regParam` and `elasticNetParam`. Then used the `.build()` method to build the grid.\n",
    "+ Defined the evaluator with `RegressionEvaluator()` with RMSE metric\n",
    "+ Defined the cross validator\n",
    "+ Trained the model using cross validation\n",
    "+ Fit training data for RMSE using `.fit()` \n",
    "+ Done predictions on `df_spark` data using `.transform()`\n",
    "+ Calculated RMSE on `df_spark` data using `.evaluate()`\n",
    "+ Printed the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e59f07-f725-4692-b5c1-5d21186048c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features_new\", labelCol=\"label\", elasticNetParam=0.5)\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[binarizer, encoder, sqltrans, assembler, pca_fit, assembler2, lr])\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 1]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "\n",
    "# Define the cross-validator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "# Fit the cross-validator\n",
    "cvModel = crossval.fit(df_spark)\n",
    "\n",
    "# Make predictions on the training set\n",
    "predictions1 = cvModel.transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26e2e507-a3b9-428e-9f95-c656a9fbd330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2124.15\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RMSE on the training set\n",
    "rmse = evaluator.evaluate(predictions1)\n",
    "print(\"RMSE: {:.2f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d62e7-83af-42da-8f32-7302c13bc45e",
   "metadata": {},
   "source": [
    "Printed residulas using `.withColumn()` by subtracting label and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de972ad0-9387-4e28-8760-81c37df67266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+------------------+\n",
      "|         residuals|      label|        prediction|\n",
      "+------------------+-----------+------------------+\n",
      "|37.593120697845734|20240.96386|20203.370739302154|\n",
      "|2118.8633006890086|20131.08434|18012.221039310993|\n",
      "|2109.3808865785104|19668.43373| 17559.05284342149|\n",
      "|1959.3565317569119|18899.27711|16939.920578243087|\n",
      "|2103.9381192680667|18442.40964|16338.471520731935|\n",
      "+------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "residuals = predictions1.withColumn(\"residuals\", predictions1.label - predictions1.prediction)\n",
    "residuals.select(\"residuals\", \"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc3e9d-6f58-474f-8e60-4c36f74ba443",
   "metadata": {},
   "source": [
    "# Streaming Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7dabf-20ee-4526-9143-b3be2ceffbfe",
   "metadata": {},
   "source": [
    "Another file was downloaded https://www4.stat.ncsu.edu/~online/datasets/power_streaming_data.csv\n",
    "\n",
    "This downloaded file was stored in stream_folder1.\n",
    "\n",
    "Schema was setup for the stream and made variables a double variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "522e5c85-5af8-4b7a-9c6a-d4cfb234e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Temperature\", DoubleType(), True),\n",
    "    StructField(\"Humidity\", DoubleType(), True),\n",
    "    StructField(\"Wind_Speed\", DoubleType(), True),\n",
    "    StructField(\"General_Diffuse_Flows\", DoubleType(), True),\n",
    "    StructField(\"Diffuse_Flows\", DoubleType(), True),\n",
    "    StructField(\"Power_Zone_1\", DoubleType(), True),\n",
    "    StructField(\"Power_Zone_2\", DoubleType(), True),\n",
    "    StructField(\"Power_Zone_3\", DoubleType(), True),\n",
    "    StructField(\"Month\", DoubleType(), True),\n",
    "    StructField(\"Hour\", DoubleType(), True)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb907ec-d71d-40e2-bdc3-cc0a6b6aaac3",
   "metadata": {},
   "source": [
    "## Reading a Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d41defd-e46c-491c-8789-a52904a991e2",
   "metadata": {},
   "source": [
    "readStream was setup and added header = True. Created a folder stream_folder2 where .csv will be sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76b3e029-c038-4cff-a9f4-51e3557a5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_data = spark \\\n",
    "    .readStream \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"stream_folder2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc55c8-739d-4b6e-b0c8-8a6572c07ffb",
   "metadata": {},
   "source": [
    "## Transform/Aggregation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6978ae4-f8eb-4060-8118-ed73beadb38b",
   "metadata": {},
   "source": [
    "Applied the model transformer to obtain predictions from the incoming data and on the resulting predictions also created a residual `.withColumn()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e71695c5-01e8-42e2-8b6d-ef9fc51f454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Temperature: double, Humidity: double, Wind_Speed: double, General_Diffuse_Flows: double, Diffuse_Flows: double, Power_Zone_1: double, Power_Zone_2: double, day_or_night: double, MonthVec: vector, label: double, features: vector, pca_features: vector, features_new: vector, prediction: double, residuals: double]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = cvModel.transform(stream_data).withColumn(\"residuals\", col(\"label\") - col(\"prediction\"))\n",
    "predictions2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429a574-b623-42be-93eb-06d91fe63769",
   "metadata": {},
   "source": [
    "In the next code, created second pipeline. The sequence of the transform were as follows:\n",
    "\n",
    "+ `Binarizer()` - In this code, doing the transformation `Binarizer()` with inputCol = \"Hour\" and outputCol = \"day_or_night\" \n",
    "\n",
    "+ `OneHotEncoder()` - In this code, created `OneHotEncoder()` estimator to convert Month into vector using inputCol = \"Month\" and outputCol= \"MonthVec\" \n",
    "\n",
    "+ `VectorAssembler()` - In this code, doing the transformation `VectorAssembler()` outputCol = \"features\" \n",
    "\n",
    "+ `PCA()` - In this code, doing the transformation `PCA()` with inputCol = \"features\" and outputCol = \"pca_features\" \n",
    "\n",
    "+ `VectorAssembler()` - Again used `VectorAssembler()` for selecting inputCol = \"pca_features\", \"day_or_night\", \"Power_Zone_1\", \"Power_Zone_2\", \"MonthVec\" and outputCol = \"features_new\" where it was selected as predictors\n",
    "\n",
    "\n",
    "+ `SQLTransformer()` - In this code, doing the transformation `SQLTransformer()` and selected all the features from the dataset and get Power_Zone_3 as label\n",
    "\n",
    "This was done by first fitting the transformer using `.fit()` and then using `.transform()` to transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fef04ba4-e154-4468-821b-e93814ad8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline 2 use this\n",
    "\n",
    "binarizer2 = Binarizer(threshold=6.5, inputCol=\"Hour\", outputCol=\"day_or_night\")\n",
    "binarizer2.transform(df_spark)\n",
    "\n",
    "# create OneHotEncoder estimator\n",
    "encoder2 = OneHotEncoder(inputCols=[\"Month\"], outputCols=[\"MonthVec\"])\n",
    "# fit and transform the data\n",
    "encoder2.fit(binarizer2.transform(df_spark)).transform(binarizer2.transform(df_spark))\n",
    "\n",
    "sqltrans2 = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                 SELECT Temperature, Humidity, Wind_Speed, General_Diffuse_Flows, Diffuse_Flows,Power_Zone_1,Power_Zone_2, day_or_night, MonthVec,\n",
    "                 Power_Zone_3, Power_Zone_3 as label FROM __THIS__\n",
    "                 \"\"\"\n",
    ")\n",
    "sqltrans2.transform(encoder2.fit(binarizer2.transform(df_spark)).transform(binarizer2.transform(df_spark)))\n",
    "\n",
    "assembler4 = VectorAssembler(inputCols=[\"Temperature\", \"Humidity\", \"Wind_Speed\", \"General_Diffuse_Flows\", \"Diffuse_Flows\"], outputCol=\"features\")\n",
    "assembler4.transform(sqltrans2.transform(encoder2.fit(binarizer2.transform(df_spark)).transform(binarizer2.transform(df_spark))))\n",
    "\n",
    "pca2 = PCA(k=5, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "assembled_data2 = assembler4.transform(sqltrans2.transform(encoder2.fit(binarizer2.transform(df_spark)).transform(binarizer2.transform(df_spark))))\n",
    "pca_fit2 = pca2.fit(assembled_data2)\n",
    "pca_fit2.transform(assembled_data2)\n",
    "\n",
    "assembler5 = VectorAssembler(inputCols=[\"pca_features\", \"day_or_night\", \"Power_Zone_1\", \n",
    "                                        \"Power_Zone_2\", \"MonthVec\"],\n",
    "                             outputCol=\"features_new\")\n",
    "assembler5.transform(pca_fit2.transform(assembled_data2))\n",
    "\n",
    "# Define second pipeline\n",
    "pipeline2 = Pipeline(stages=[binarizer2, encoder2, assembler4, pca2, assembler5, sqltrans2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd0d91-8e82-4391-9477-97dc56840b95",
   "metadata": {},
   "source": [
    "Fitted second pipeline on the data frame `df_spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71d80a42-df60-4f85-b2ee-bf517edbd522",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamdf = pipeline2.fit(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf9027-fbc2-43d5-a7ee-d1208d92effb",
   "metadata": {},
   "source": [
    "Applied second pipeline on the streaming data source `stream_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7faf2628-d982-4b41-91a9-e291c849e2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Temperature: double, Humidity: double, Wind_Speed: double, General_Diffuse_Flows: double, Diffuse_Flows: double, Power_Zone_1: double, Power_Zone_2: double, day_or_night: double, MonthVec: vector, Power_Zone_3: double, label: double]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data = streamdf.transform(stream_data)\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925127b-6e08-4cb9-8798-314f79172de8",
   "metadata": {},
   "source": [
    "## Writing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa11022-355c-4a5c-a2be-7cb704cfe790",
   "metadata": {},
   "source": [
    "Combined it with a `.join()` transformed_data and predictions2 with inner join on label. Wrote a stream with `.writeStream` to the console using the append output mode. Started the query with `.start()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4596d3a8-32b4-472c-9913-64e41127491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the streaming query\n",
    "stream_query = transformed_data \\\n",
    "    .join(predictions2,\"label\",\"inner\")\\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb541a-8522-463f-a231-9a9f324172a4",
   "metadata": {},
   "source": [
    "## Produce Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4757c49-fabb-4d9e-81b8-185060385dc2",
   "metadata": {},
   "source": [
    "The streaming data downloaded and placed in stream_folder1.\n",
    "\n",
    "Wrote a loop of 50 iterations to:\n",
    "\n",
    "+ Randomly sampled three rows and output those to a .csv file in the folder\n",
    "+ Wrote a Pause for 10 seconds in between outputting of data sets\n",
    "+ Submitted this loop in a python console.\n",
    "+ While the loop ran, checked log (command prompt or terminal window) and the output being made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d54110-7a9e-47fb-b2d1-cf4fd6d78e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in some data to sample from\n",
    "import pandas as pd\n",
    "power_stream = pd.read_csv(\"stream_folder1/power_streaming_data.csv\")\n",
    "\n",
    "#Now a for loop to sample a few rows and output them to a data set\n",
    "#Pause for 10 sec \n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "for i in range(0,50):\n",
    "    #randomly sample a few rows\n",
    "    temp = power_stream.loc[np.random.randint(power_stream.shape[0], size = 3)]\n",
    "    temp[\"timestamp\"] = [time.strftime(\"%H:%M:%S\", time.localtime())]*3\n",
    "    temp.to_csv(\"stream_folder2/power_stream\" + str(i) + \".csv\", index = False, header = False)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc309ff-9a65-4a28-b30f-3b2437ce736e",
   "metadata": {},
   "source": [
    "stopped the query using `.stop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a31bc617-ce5a-427b-a237-a2984136e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277618f-f4af-4eb4-a101-197e63ba6284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
